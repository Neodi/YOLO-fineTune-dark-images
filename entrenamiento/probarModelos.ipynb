{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.49 🚀 Python-3.9.20 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "Model summary (fused): 168 layers, 3,007,988 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/david/vision_computador/entrega3/yolo/data/labels/test.cache... 748 images, 0 backgrounds, 0 corrupt: 100%|██████████| 748/748 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/david/vision_computador/entrega3/yolo/data/images/test/2015_02635.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/david/vision_computador/entrega3/yolo/data/images/test/2015_03362.jpg: corrupt JPEG restored and saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 47/47 [00:04<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        748       2380        0.7      0.489      0.564      0.351\n",
      "               Bicycle         79        112      0.761      0.598      0.685      0.444\n",
      "                  Boat         71        139      0.721      0.409      0.507      0.255\n",
      "                Bottle         75        126      0.582       0.46      0.513      0.326\n",
      "                   Bus         57         83      0.823      0.616      0.678      0.521\n",
      "                   Car        144        316       0.77      0.481      0.581      0.359\n",
      "                   Cat         77        100      0.563        0.5      0.481      0.309\n",
      "                 Chair        130        274      0.744      0.393      0.507      0.281\n",
      "                   Cup         84        129       0.66      0.457      0.502      0.321\n",
      "                   Dog         88        100      0.674      0.599      0.677      0.466\n",
      "             Motorbike         55        111      0.698      0.477      0.576      0.337\n",
      "                People        254        750      0.772       0.48      0.597      0.319\n",
      "                 Table         99        140      0.632      0.393      0.458       0.27\n",
      "Speed: 0.1ms preprocess, 1.5ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/nano\u001b[0m\n",
      "Ultralytics 8.3.49 🚀 Python-3.9.20 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "Model summary (fused): 168 layers, 11,130,228 parameters, 0 gradients, 28.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/david/vision_computador/entrega3/yolo/data/labels/test.cache... 748 images, 0 backgrounds, 0 corrupt: 100%|██████████| 748/748 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/david/vision_computador/entrega3/yolo/data/images/test/2015_02635.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/david/vision_computador/entrega3/yolo/data/images/test/2015_03362.jpg: corrupt JPEG restored and saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 47/47 [00:05<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        748       2380      0.622      0.447      0.501      0.295\n",
      "               Bicycle         79        112      0.661      0.589      0.649      0.409\n",
      "                  Boat         71        139      0.661      0.324      0.407      0.195\n",
      "                Bottle         75        126      0.532      0.548      0.541      0.296\n",
      "                   Bus         57         83       0.72      0.559      0.588      0.435\n",
      "                   Car        144        316      0.813      0.439      0.586      0.339\n",
      "                   Cat         77        100      0.442      0.413      0.361      0.205\n",
      "                 Chair        130        274      0.603       0.35      0.426      0.238\n",
      "                   Cup         84        129      0.607      0.411      0.474      0.309\n",
      "                   Dog         88        100      0.563       0.52      0.555      0.344\n",
      "             Motorbike         55        111      0.698      0.351      0.473      0.255\n",
      "                People        254        750      0.672      0.495       0.57      0.304\n",
      "                 Table         99        140      0.493      0.371      0.381      0.213\n",
      "Speed: 0.1ms preprocess, 2.9ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/small\u001b[0m\n",
      "Ultralytics 8.3.49 🚀 Python-3.9.20 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "Model summary (fused): 218 layers, 25,846,708 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/david/vision_computador/entrega3/yolo/data/labels/test.cache... 748 images, 0 backgrounds, 0 corrupt: 100%|██████████| 748/748 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/david/vision_computador/entrega3/yolo/data/images/test/2015_02635.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/david/vision_computador/entrega3/yolo/data/images/test/2015_03362.jpg: corrupt JPEG restored and saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 47/47 [00:21<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        748       2380      0.777      0.704      0.778       0.53\n",
      "               Bicycle         79        112      0.897      0.795      0.876      0.635\n",
      "                  Boat         71        139      0.719      0.663      0.714        0.4\n",
      "                Bottle         75        126      0.767      0.732      0.812      0.552\n",
      "                   Bus         57         83      0.889      0.769      0.868      0.698\n",
      "                   Car        144        316      0.823      0.804      0.877      0.616\n",
      "                   Cat         77        100      0.662       0.61      0.654      0.429\n",
      "                 Chair        130        274      0.735      0.569      0.665      0.447\n",
      "                   Cup         84        129      0.762      0.783        0.8       0.55\n",
      "                   Dog         88        100      0.717        0.7      0.774      0.563\n",
      "             Motorbike         55        111      0.833       0.73      0.842      0.553\n",
      "                People        254        750      0.832      0.726      0.819      0.494\n",
      "                 Table         99        140      0.689      0.564       0.63      0.419\n",
      "Speed: 0.4ms preprocess, 23.4ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/medium\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# nano\n",
    "model1 = YOLO('runs/detect/yolov8n_exp1/weights/best.pt')\n",
    "results1 = model1.val(data='../data/data.yaml', split='test', name = 'nano')\n",
    "\n",
    "# small\n",
    "model2 = YOLO('runs/detect/yolov8s_exp1/weights/best.pt')\n",
    "results2 = model2.val(data='../data/data.yaml', split='test', name = 'small')\n",
    "\n",
    "# medium\n",
    "model3 = YOLO('collab_m.pt')\n",
    "results3 = model3.val(data='../data/data.yaml', split='test', name = 'medium')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nano Model Metrics:\n",
      "{'mAP50': np.float64(0.5636658203276061),\n",
      " 'mAP50-95': np.float64(0.3505707116974897),\n",
      " 'mAP50-95 per category': array([    0.44423,     0.25453,     0.32564,     0.52069,     0.35912,     0.30853,     0.28104,     0.32116,     0.46593,     0.33714,     0.31878,     0.27006]),\n",
      " 'mAP75': np.float64(0.3619427112543798)}\n",
      "\n",
      "Small Model Metrics:\n",
      "{'mAP50': np.float64(0.5010412003120025),\n",
      " 'mAP50-95': np.float64(0.295069267498358),\n",
      " 'mAP50-95 per category': array([    0.40948,     0.19472,     0.29621,     0.43461,     0.33914,     0.20471,     0.23759,     0.30871,     0.34374,     0.25535,     0.30377,     0.21282]),\n",
      " 'mAP75': np.float64(0.30813501216956485)}\n",
      "\n",
      "Medium Model Metrics:\n",
      "{'mAP50': np.float64(0.777590601718289),\n",
      " 'mAP50-95': np.float64(0.529567329436582),\n",
      " 'mAP50-95 per category': array([    0.63474,     0.39993,     0.55189,     0.69773,     0.61579,     0.42918,     0.44661,     0.55021,     0.56314,     0.55266,     0.49376,     0.41915]),\n",
      " 'mAP75': np.float64(0.5954580742862144)}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "metrics = results1 \n",
    "print(\"Nano Model Metrics:\")\n",
    "pprint({\n",
    "    \"mAP50-95\": metrics.box.map,\n",
    "    \"mAP50\": metrics.box.map50,\n",
    "    \"mAP75\": metrics.box.map75,\n",
    "    \"mAP50-95 per category\": metrics.box.maps\n",
    "})\n",
    "\n",
    "metrics = results2 \n",
    "print(\"\\nSmall Model Metrics:\")\n",
    "pprint({\n",
    "    \"mAP50-95\": metrics.box.map,\n",
    "    \"mAP50\": metrics.box.map50,\n",
    "    \"mAP75\": metrics.box.map75,\n",
    "    \"mAP50-95 per category\": metrics.box.maps\n",
    "})\n",
    "\n",
    "metrics = results3\n",
    "print(\"\\nMedium Model Metrics:\")\n",
    "pprint({\n",
    "    \"mAP50-95\": metrics.box.map,\n",
    "    \"mAP50\": metrics.box.map50,\n",
    "    \"mAP75\": metrics.box.map75,\n",
    "    \"mAP50-95 per category\": metrics.box.maps\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
